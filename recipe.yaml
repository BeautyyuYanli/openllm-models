"phi3:3.8b-mini-instruct-4k-fp16":
  alias:
    - latest
    - 3.8b
    - instruct
    - mini
  project: vllm-chat
  service_config:
    name: phi3
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-t4
  engine_config:
    model: microsoft/Phi-3-mini-4k-instruct
    max_model_len: 4096
    dtype: half
  chat_template: phi-3
"llama2:7b-chat-fp16":
  alias:
    - latest
    - 7b
    - 7b-chat
  project: vllm-chat
  service_config:
    name: llama2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-t4
  engine_config:
    model: meta-llama/Llama-2-7b-chat-hf
    max_model_len: 1024
  chat_template: llama-2-chat
"llama2:7b-chat-awq-4bit":
  alias:
    - 7b-chat-4bit
  project: vllm-chat
  service_config:
    name: llama2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: TheBloke/Llama-2-7B-Chat-AWQ
    max_model_len: 1024
    quantization: awq
    enforce_eager: true
  chat_template: llama-2-chat
"mistral:7b-instruct-awq-4bit":
  alias: []
  project: vllm-chat
  service_config:
    name: mistral
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: TheBloke/Mistral-7B-Instruct-v0.1-AWQ
    max_model_len: 1024
    quantization: awq
    enforce_eager: true
    dtype: half
  chat_template: mistral-instruct
"mistral:7b-instruct-fp16":
  alias:
    - 7b
    - 7b-instruct
    - 7b-instruct-v0.1
  project: vllm-chat
  service_config:
    name: mistral
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-l4
  engine_config:
    model: mistralai/Mistral-7B-Instruct-v0.1
    max_model_len: 1024
    enforce_eager: true
    dtype: half
  chat_template: mistral-instruct
